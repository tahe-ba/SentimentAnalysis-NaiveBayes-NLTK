{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "\n",
    "data_path = './data/'\n",
    "# read local files\n",
    "\n",
    "Custom_Header = [\"Sentence\", \"Sentiment\"]\n",
    "data_amazon = pd.read_csv(data_path+'amazon.txt', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                          header=None, names=Custom_Header)\n",
    "# imdb pandas reading only 748 rows of 1000 fixed by adding quoting=csv.QUOTE_NONE\n",
    "data_imdb = pd.read_csv(data_path+'imdb.txt', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                        header=None, names=Custom_Header)\n",
    "data_yelp = pd.read_csv(data_path+'yelp.txt', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                        header=None, names=Custom_Header)\n",
    "\n",
    "# stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "#               \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "#               \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\n",
    "#               \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\",\n",
    "#               \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
    "#               \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
    "#               \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
    "#               \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
    "#               \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
    "#               \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "# print(data.head())\n",
    "# print(data.info())\n",
    "\n",
    "# loop through data\n",
    "# for i in range(0, len(data)):\n",
    "#     if data.iloc[i,1] == 1:\n",
    "#         #print(data.iloc[i,0], 'Positive')\n",
    "#         positive = positive + 1\n",
    "#     else:\n",
    "#         #print(data.iloc[i,0], 'Negative')\n",
    "#         negative = negative + 1\n",
    "\n",
    "# data[column][line] not working after adding column names\n",
    "# column = {0,1} => get sentence or sentiment\n",
    "# line = {0,999} => which line\n",
    "# print(data[0][500])\n",
    "# Alternative =>\n",
    "# print(data['Sentence'][500])\n",
    "# print(data['Sentiment'][500])\n",
    "\n",
    "# if we use data.iloc[x,y]\n",
    "# x = line number {0,999}\n",
    "# y = column number {0,1}\n",
    "# if y = 0 => get sentence\n",
    "# if y = 1 => get sentiment\n",
    "#print(data.iloc[500, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_array = stopwords.words('english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_words(data):\n",
    "    # create an empty list to store the clean words\n",
    "    clean_words = []\n",
    "    # loop through every sentence in the data\n",
    "    for i in range(0, len(data)):\n",
    "        # remove punctuation from the sentence\n",
    "        text = data[\"Sentence\"][i]\n",
    "        text = ''.join([c for c in text if c not in string.punctuation])\n",
    "        # tokenize the sentence into words\n",
    "        tokens = word_tokenize(text)\n",
    "        # remove stop words from the tokens\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # append the tokens to the empty list\n",
    "        clean_words.append(tokens)\n",
    "    return clean_words\n",
    "\n",
    "# create a list of tuples containing words in the sentences and their labels\n",
    "def create_word_features(words):\n",
    "    my_dict = dict([(word, True) for word in words])\n",
    "    return my_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to tokenize amazon data with list comprehension:  0.14963722229003906\n",
      "Time taken to tokenize amazon data without list comprehension:  0.1040182113647461\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('punkt')\n",
    "# tokenize the sentences () into words and remove punctuation and stop words\n",
    "# tokenize the words every data_amazon row using word_tokenize \n",
    "\n",
    "alt_start = time.time()\n",
    "# with list comprehension\n",
    "alt_tokens_amazon = set(word.lower() for words in data_amazon['Sentence'] for word in word_tokenize(words) if word.lower() not in stop_words_array and word.lower() not in string.punctuation)\n",
    "alt_end = time.time()\n",
    "\n",
    "print(\"Time taken to tokenize amazon data with list comprehension: \", alt_end-alt_start)\n",
    "for_start = time.time()\n",
    "# without list comprehension \n",
    "amazon_tokens = set()\n",
    "for words in data_amazon[\"Sentence\"]:\n",
    "    for word in word_tokenize(words):\n",
    "        if word.lower() not in stop_words:\n",
    "            if word.lower() not in string.punctuation:\n",
    "                amazon_tokens.add(word.lower())\n",
    "for_end = time.time()\n",
    "print(\"Time taken to tokenize amazon data without list comprehension: \", for_end-for_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken to tokenize amazon data with list comprehension:  0.19963722229003906\n",
    "Time taken to tokenize amazon data without list comprehension:  0.1040182113647461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_start = time.time()\n",
    "amazon_train = [({word: (word in word_tokenize(data_amazon['Sentence'][i].lower()) and word.lower() not in stop_words_array)\n",
    "                  for word in amazon_tokens}, data_amazon['Sentiment'][i]) for i in range(0, len(data_amazon))]\n",
    "alt_end = time.time()\n",
    "print(\"Time taken to create training data with list comprehension: \", alt_end-alt_start)\n",
    "\n",
    "# print(len(amazon_train))\n",
    "# print(amazon_train[0])\n",
    "for_start = time.time()\n",
    "amazon_train_alt = []\n",
    "for i in range(0, len(data_amazon)):\n",
    "    # This dictionary with the word as key and True if the word is in the word_tokenize and False if it is not\n",
    "    amazon_dict = {}\n",
    "    # we compare each word in the amazon_tokens to each word in the data_amazon['Sentence'][i] and if the word is in the data_amazon['Sentence'][i]\n",
    "    # we add it to the dictionary with the value True and if it is not in the data_amazon['Sentence'][i] we add it to the dictionary with the value False\n",
    "    for word in amazon_tokens:\n",
    "        if word in word_tokenize(data_amazon['Sentence'][i].lower()) and word.lower() not in stop_words_array and word not in string.punctuation:\n",
    "            amazon_dict[word] = True\n",
    "        else:\n",
    "            amazon_dict[word] = False\n",
    "    # create a tuple with the dictionary and the sentiment\n",
    "    amazon_tuple = (amazon_dict, data_amazon['Sentiment'][i])\n",
    "    # add the tuple to the amazon_train list\n",
    "    amazon_train_alt.append(amazon_tuple)\n",
    "for_end = time.time()\n",
    "print(\"Time taken to create training data without list comprehension: \",\n",
    "      for_end-for_start)\n",
    "\n",
    "# print(len(amazon_train_alt))\n",
    "# print(amazon_train_alt[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken to create training data with list comprehension:  176.76983428001404\n",
    "Time taken to create training data without list comprehension:  178.1755678653717\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(amazon_train)\n",
    "amazon_train_x = amazon_train[:int(len(amazon_train)*0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#create model using  NaiveBayesClassifier\n",
    "model = nltk.NaiveBayesClassifier.train(amazon_train_x)\n",
    "model.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(amazon_train)\n",
    "amazon_test_x = amazon_train[int(len(amazon_train)*0.2):]\n",
    "acc=nltk.classify.accuracy(model, amazon_test_x)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests=['I really like it', \n",
    "       'I do not think this is good one', \n",
    "       'this is good one',\n",
    "       'I hate the show!']\n",
    "for test in tests:\n",
    "    print(test, model.classify({word: (word in word_tokenize(test.lower()))\n",
    "                 for word in amazon_tokens}))\n",
    "                 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
