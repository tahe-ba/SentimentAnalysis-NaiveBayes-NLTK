{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qwb3YTH0lIIJ"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pandas as pd\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import NaiveBayesClassifier\n",
        "from nltk import classify\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ivFccf5KlIIN",
        "outputId": "99f607d8-78c9-46cf-dddb-4856b45ecf5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UdmIfBeolIIO"
      },
      "outputs": [],
      "source": [
        "# file path to the txt files\n",
        "data_path = './data/'\n",
        "\n",
        "# load csv files into pandas dataframes\n",
        "Custom_Header = [\"Sentence\", \"Sentiment\"]\n",
        "data_amazon = pd.read_csv(data_path+'amazon.txt', sep='\\t', quoting=csv.QUOTE_NONE,\n",
        "                          header=None, names=Custom_Header)\n",
        "data_imdb = pd.read_csv(data_path+'imdb.txt', sep='\\t', quoting=csv.QUOTE_NONE,\n",
        "                        header=None, names=Custom_Header)\n",
        "data_yelp = pd.read_csv(data_path+'yelp.txt', sep='\\t', quoting=csv.QUOTE_NONE,\n",
        "                        header=None, names=Custom_Header)\n",
        "\n",
        "# print(data_amazon.head(), data_imdb.head(), data_yelp.head())\n",
        "# print(data_amazon.shape, data_imdb.shape, data_yelp.shape)\n",
        "# print(data_amazon.info(), data_imdb.info(), data_yelp.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdI6MJjRlIIP"
      },
      "source": [
        "##### Stop words are words that do not add much meaning to a sentence and can be removed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bAmuajgUlIIe"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words_array = stopwords.words('english')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODWuJU4DlIIf"
      },
      "source": [
        "##### Tokenize every dataset reviews and remove stop words and punctuations\n",
        "1. Create a new set of words from the words in the dataset.\n",
        "2. For each sentence in the dataset tokenize the words and remove stop words and punctuations.\n",
        "3. Create a new set of words from the words in the dataset.\n",
        "4. It can be written as:\n",
        "```python\n",
        "tokens_ = set()\n",
        "for words in data[\"Sentence\"]:\n",
        "    for word in word_tokenize(words):\n",
        "        if word.lower() not in stop_words:\n",
        "            if word.lower() not in string.punctuation:\n",
        "                tokens.add(word.lower())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f0UowxkolIIg"
      },
      "outputs": [],
      "source": [
        "amazon_tokens = set(word.lower() for words in data_amazon['Sentence'] for word in word_tokenize(\n",
        "    words) if word.lower() not in stop_words_array and word.lower() not in string.punctuation)\n",
        "\n",
        "imdb_tokens = set(word.lower() for words in data_imdb['Sentence'] for word in word_tokenize(\n",
        "    words) if word.lower() not in stop_words_array and word.lower() not in string.punctuation)\n",
        "\n",
        "yelp_tokens = set(word.lower() for words in data_yelp['Sentence'] for word in word_tokenize(\n",
        "    words) if word.lower() not in stop_words_array and word.lower() not in string.punctuation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_tokenizer(sentence):\n",
        "    # remove stop words and punctuations\n",
        "    words = word_tokenize(sentence)\n",
        "    words = [word.lower() for word in words if word.lower()\n",
        "             not in stop_words_array and word.lower() not in string.punctuation]\n",
        "    return words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 1795)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
        "tfidf = vectorizer.fit_transform(data_amazon['Sentence'])\n",
        "# print(tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(data_amazon['Sentiment'])\n",
        "print(le.classes_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.825\n",
            "[[70 23]\n",
            " [12 95]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.75      0.80        93\n",
            "           1       0.81      0.89      0.84       107\n",
            "\n",
            "    accuracy                           0.82       200\n",
            "   macro avg       0.83      0.82      0.82       200\n",
            "weighted avg       0.83      0.82      0.82       200\n",
            "\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# predict the sentiment of a sentence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "classifier = MultinomialNB()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    tfidf, data_amazon['Sentiment'], test_size=0.2, random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentiment(sentence):\n",
        "    # transform the sentence into tfidf vector\n",
        "    tfidf = vectorizer.transform([sentence])\n",
        "    # predict the sentiment\n",
        "    return classifier.predict(tfidf)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "print(predict_sentiment('I hate this product'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IxOKccYlIIh"
      },
      "source": [
        "We create a list of tuples. Each tuple contains a dictionary and a string.\n",
        "\n",
        "The dictionary contains the words in the token and the string is the sentiment of the review\n",
        "1. Creates a list of dictionaries. Each dictionary contains a word as a key and a boolean value.\n",
        "2. The boolean value is true if the word is in the sentence and false if not.\n",
        "3. Creates a list of tuples. Each tuple contains a dictionary and the sentiment\n",
        "4. It can be written as :\n",
        "```python\n",
        "train_alt = []\n",
        "for i in range(0, len(data)):\n",
        "    dict_ = {}\n",
        "    for word in tokens_:\n",
        "        if word in word_tokenize(data['Sentence'][i].lower()) and word.lower() not in stop_words_array:\n",
        "            dict_[word] = True\n",
        "        else:\n",
        "            dict_[word] = False\n",
        "    # create a tuple with the dictionary and the sentiment\n",
        "    tuple_ = (dict_, data['Sentiment'][i])\n",
        "    # add the tuple to the train_ array\n",
        "    train_alt.append(tuple_)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xJq5WGT_lIIi"
      },
      "outputs": [],
      "source": [
        "amazon_train = [({word: (word in word_tokenize(data_amazon['Sentence'][i].lower()) and word.lower() not in stop_words_array)\n",
        "                  for word in amazon_tokens}, data_amazon['Sentiment'][i]) for i in range(0, len(data_amazon))]\n",
        "imdb_train = [({word: (word in word_tokenize(data_imdb['Sentence'][i].lower()) and word.lower() not in stop_words_array)\n",
        "                for word in imdb_tokens}, data_imdb['Sentiment'][i]) for i in range(0, len(data_imdb))]\n",
        "yelp_train = [({word: (word in word_tokenize(data_yelp['Sentence'][i].lower()) and word.lower() not in stop_words_array)\n",
        "                for word in yelp_tokens}, data_yelp['Sentiment'][i]) for i in range(0, len(data_yelp))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPyEj_RplIIj"
      },
      "source": [
        "##### For bigger data set we merge the data sets together\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u7WmgXHJlIIj"
      },
      "outputs": [],
      "source": [
        "data_train = amazon_train + imdb_train + yelp_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBSE2glflIIk"
      },
      "source": [
        "##### FUNCTION to save the data_train to a pickle file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rPN4w5H1lIIk"
      },
      "outputs": [],
      "source": [
        "def save_data_train(data, file_name):\n",
        "    with open(file_name, 'wb') as handle:\n",
        "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJs2GVvklIIl"
      },
      "source": [
        "##### Load the data_train from the pickle file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2THL0Yx5lIIl"
      },
      "outputs": [],
      "source": [
        "save_data_train(data_train, 'data_train.pickle')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6w-p8ZclIIl"
      },
      "source": [
        "##### FUNCTION to load the data_train from the pickle file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VZ9ecMMzlIIm"
      },
      "outputs": [],
      "source": [
        "def load_data_train(file_name):\n",
        "    with open(file_name, 'rb') as handle:\n",
        "        data_train = pickle.load(handle)\n",
        "    return data_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXrSuL_wlIIm"
      },
      "source": [
        "##### Load the data_train from the pickle file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "L5vpLjAGlIIm"
      },
      "outputs": [],
      "source": [
        "data_train = load_data_train('data_train.pickle')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FXPUhLlIIm"
      },
      "source": [
        "##### Shuffle the data_train to randomize the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nvRdqkujlIIn"
      },
      "outputs": [],
      "source": [
        "random.shuffle(data_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr8rw2VQlIIn"
      },
      "source": [
        "##### Split the data_train into training and testing data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xWX70WbtlIIn"
      },
      "outputs": [],
      "source": [
        "data_train_x = data_train[:int(len(data_train)*0.8)]\n",
        "data_test_x = data_train[int(len(data_train)*0.8):]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E5vaWkUlIIn"
      },
      "source": [
        "##### Create the model using NaiveBayesClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_UfM5Nh2lIIn",
        "outputId": "aeacfcb5-02a1-4c6f-d4fe-8a6a9ca19c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Informative Features\n",
            "                   great = True                1 : 0      =     28.3 : 1.0\n",
            "               excellent = True                1 : 0      =     26.5 : 1.0\n",
            "                   worst = True                0 : 1      =     21.6 : 1.0\n",
            "                     bad = True                0 : 1      =     18.7 : 1.0\n",
            "                   waste = True                0 : 1      =     17.1 : 1.0\n",
            "                terrible = True                0 : 1      =     15.2 : 1.0\n",
            "                    nice = True                1 : 0      =     14.3 : 1.0\n",
            "                   happy = True                1 : 0      =     12.8 : 1.0\n",
            "                 amazing = True                1 : 0      =     12.6 : 1.0\n",
            "                    fine = True                1 : 0      =     11.4 : 1.0\n"
          ]
        }
      ],
      "source": [
        "model = NaiveBayesClassifier.train(data_train_x)\n",
        "model.show_most_informative_features()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO3fkE_3lIIo"
      },
      "source": [
        "##### compare the accuracy of the model to the test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BRMtVP43lIIo",
        "outputId": "577f207b-b864-4a3b-d8a3-f830a4955430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.47833333333333333\n"
          ]
        }
      ],
      "source": [
        "acc = classify.accuracy(model, data_test_x)\n",
        "print(\"Accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srV-X5KKlIIo"
      },
      "source": [
        "##### FUNCTION to save the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DvfSUTSolIIo"
      },
      "outputs": [],
      "source": [
        "def save_model(model_, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(model_, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPNKlABalIIo"
      },
      "source": [
        "##### Save the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uEsSK3U7lIIp"
      },
      "outputs": [],
      "source": [
        "save_model(model, 'model.pickle')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voUSapGVlIIp"
      },
      "source": [
        "##### FUNICTION to load the model from disk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cFGjQ_MflIIp"
      },
      "outputs": [],
      "source": [
        "def load_model(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BrBPnumlIIp"
      },
      "source": [
        "##### Load the model from pickle file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CCqdTKGMlIIp"
      },
      "outputs": [],
      "source": [
        "newmodel = load_model('model.pickle')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jNFDzeDlIIp"
      },
      "source": [
        "##### FUNCTION to predict the sentiment of a sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WtXrDbAOlIIq"
      },
      "outputs": [],
      "source": [
        "def predict(sentence):\n",
        "    return newmodel.classify(dict([token, True] for token in word_tokenize(sentence.lower())))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQNacJAslIIq"
      },
      "source": [
        "##### Test the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KwQrk74XlIIq",
        "outputId": "c6cbb532-60c1-464a-b33b-9a8d13749f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "test_sentence = \"I love this movie\"\n",
        "test_sentence2 = \"I hate this movie\"\n",
        "\n",
        "print(predict(test_sentence))\n",
        "print(predict(test_sentence2))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
